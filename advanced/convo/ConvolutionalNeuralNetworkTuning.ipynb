{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Read through the code in the following sections:\n",
    "    * [MNIST Data](#MNIST-Data)\n",
    "    * [Convolutional Neural Network Model](#Convolutional-Neural-Network-Model)\n",
    "    * [Train Model](#Train-Model)\n",
    "    * [Validation](#Validation)\n",
    "0. Complete the [Exercise](#Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow matplotlib graphics to display in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# import numpy, for image dimension manipulation\n",
    "import numpy\n",
    "\n",
    "# import validation methods from scikit-learn\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# import the dataset and neural network layers from keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "[MNIST](https://en.wikipedia.org/wiki/MNIST_database) is a famous dataset of images of handwritten numbers. The goal is to be able to figure out which number is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants to describe the MNIST images\n",
    "NUM_ROWS = 28 \n",
    "NUM_COLUMNS = 28\n",
    "NUM_COLORS = 1\n",
    "IMG_SHAPE = (NUM_ROWS, NUM_COLUMNS, NUM_COLORS)\n",
    "\n",
    "# constant to describe the MNIST output labels\n",
    "# there are ten different numbers, 0-9\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "((images_train, labels_train), (images_test, labels_test)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one particular image and its label, to better understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1434838b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(images_train[17], cmap='twilight_shifted_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "model.add(Conv2D(filters=6, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 1.6090\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.5035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1436ba3a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_train = numpy.expand_dims(images_train, axis=3)\n",
    "\n",
    "# the labels need to be one-hot encoded, to match the ten outputs of our model\n",
    "labels_onehot_train = to_categorical(labels_train)\n",
    "\n",
    "# set up the model to be ready for training\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(images_3d_train, labels_onehot_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_test = numpy.expand_dims(images_test, axis=3)\n",
    "\n",
    "# get the predictions from the model\n",
    "predictions_test_onehot = model.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test = numpy.argmax(predictions_test_onehot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5451"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.50462487, 0.61304623, 0.77220077, 0.43245175, 0.55588453,\n",
       "        0.56422018, 0.66868932, 0.73495248, 0.37167139, 0.59854015]),\n",
       " array([0.50102041, 0.85286344, 0.19379845, 0.5990099 , 0.76476578,\n",
       "        0.13789238, 0.57515658, 0.6770428 , 0.67351129, 0.40634291]),\n",
       " array([0.50281618, 0.71333825, 0.30983734, 0.50228311, 0.64380626,\n",
       "        0.22162162, 0.61840629, 0.70481013, 0.47900694, 0.48406139]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get precision, recall, f-score, and number of examples of each digit\n",
    "# can you see which digits are easiest for the model, and which are hardest? looks to be struggling with 5 and 8\n",
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Can you get the model to perform better? Try adding more layers, or taking them away. \n",
    "Take a look at the documentation for the [convolutional](https://keras.io/layers/convolutional/) and [dense](https://keras.io/layers/core/) layers and the [sequential model](https://keras.io/models/sequential/) to understand the options that you have and try out different things.\n",
    "\n",
    "It might also be a good idea to find examples posted online of networks that did well with MNIST and try out some of the configuration they used. Make sure you cite any sources you use!\n",
    "\n",
    "Take notes of what performance you get from different configurations.\n",
    "\n",
    "\n",
    "Comment on what patterns you observed in terms of what changes helped your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#   def   #\n",
    "###########\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=4, kernel_size=6, strides=2, padding='same')) # focus on bigger features, \n",
    "# model2.add(Dropout(0.2))\n",
    "model2.add(Conv2D(filters=8, kernel_size=2, strides=2, padding='same')) # then look closer\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model2.add(Dense(units=NUM_CLASSES, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 1.2386\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.7366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x143db0e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#############\n",
    "#   train   #\n",
    "#############\n",
    "\n",
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_train = numpy.expand_dims(images_train, axis=3)\n",
    "\n",
    "# the labels need to be one-hot encoded, to match the ten outputs of our model\n",
    "labels_onehot_train = to_categorical(labels_train)\n",
    "\n",
    "# set up the model to be ready for training\n",
    "model2.compile(optimizer=Adam(), loss='categorical_crossentropy') # Adam converges faster, better with only 2 epochs\n",
    "\n",
    "# fit the model to the training data\n",
    "model2.fit(images_3d_train, labels_onehot_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "#   score   #\n",
    "#############\n",
    "\n",
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_test = numpy.expand_dims(images_test, axis=3)\n",
    "\n",
    "# get the predictions from the model\n",
    "predictions_test_onehot = model2.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test = numpy.argmax(predictions_test_onehot, axis=1)\n",
    "\n",
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "\n",
    "Top score: 0.808\n",
    "\n",
    "---\n",
    "\n",
    "Adam seems to work best\n",
    "\n",
    "More convolutional layers seems to work worse, maybe because we only have two epochs\n",
    "\n",
    "Removing the dropout made it work better, at least with only a few layers\n",
    "\n",
    "More dense layers seemed to work worse\n",
    "\n",
    "looking large then small seemed to work better than looking small then large\n",
    "\n",
    "Less filters seems to work better! Final try pushed previous learnings to the extreme, and worked best!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=12, kernel_size=3, strides=2, padding='same'))\n",
    "# model2.add(Conv2D(filters=6, kernel_size=6, strides=2, padding='same'))\n",
    "# model2.add(Dropout(0.3))\n",
    "# model2.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "\n",
    "# # dense layers to consolidate information\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(units=12, activation='tanh'))\n",
    "# model2.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# # 0.65, adam op\n",
    "\n",
    "\n",
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "# model2.add(Conv2D(filters=12, kernel_size=2, strides=2, padding='same'))\n",
    "# model2.add(Dropout(0.3))\n",
    "# model2.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "# model2.add(Dropout(0.3))\n",
    "# model2.add(Conv2D(filters=6, kernel_size=6, strides=2, padding='same'))\n",
    "\n",
    "# # dense layers to consolidate information\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(units=12, activation='tanh'))\n",
    "# model2.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# # # 0.75, adam op\n",
    "\n",
    "\n",
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=12, kernel_size=6, strides=2, padding='same'))\n",
    "# model2.add(Conv2D(filters=16, kernel_size=2, strides=2, padding='same'))\n",
    "# model2.add(Dropout(0.3))\n",
    "# model2.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "\n",
    "# # dense layers to consolidate information\n",
    "# model2.add(Flatten())\n",
    "# # model2.add(Dense(units=12, activation='tanh'))\n",
    "# model2.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# # # 0.7\n",
    "\n",
    "\n",
    "\n",
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=6, kernel_size=6, strides=2, padding='same'))\n",
    "# model2.add(Conv2D(filters=16, kernel_size=3, strides=2, padding='same'))\n",
    "# model2.add(Dropout(0.3))\n",
    "# model2.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "\n",
    "# # dense layers to consolidate information\n",
    "# model2.add(Flatten())\n",
    "# # model2.add(Dense(units=12, activation='tanh'))\n",
    "# model2.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# # # 0.74 \n",
    "\n",
    "\n",
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=6, kernel_size=6, strides=2, padding='same'))\n",
    "# # model2.add(Dropout(0.2))\n",
    "# model2.add(Conv2D(filters=12, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "# # # 0.76\n",
    "\n",
    "\n",
    "# model2 = Sequential()\n",
    "\n",
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=6, kernel_size=6, strides=2, padding='same')) # focus on bigger features, \n",
    "# # model2.add(Dropout(0.2))\n",
    "# model2.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same')) # then look closer\n",
    "\n",
    "# # dense layers to consolidate information\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# # output layer to make the final decision on which number it is\n",
    "# model2.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# # # 0.77\n",
    "\n",
    "\n",
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=4, kernel_size=6, strides=2, padding='same')) # focus on bigger features, \n",
    "# # model2.add(Dropout(0.2))\n",
    "# model2.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same')) # then look closer\n",
    "\n",
    "# # dense layers to consolidate information\n",
    "# model2.add(Flatten())\n",
    "# model2.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# # # 0.78\n",
    "\n",
    "# # convolutional layers\n",
    "# model2.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "#                  filters=4, kernel_size=6, strides=2, padding='same')) # focus on bigger features, \n",
    "# # model2.add(Dropout(0.2))\n",
    "# model2.add(Conv2D(filters=8, kernel_size=2, strides=2, padding='same')) # then look closer\n",
    "\n",
    "# # # 0.808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
